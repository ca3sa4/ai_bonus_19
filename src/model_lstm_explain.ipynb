{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral']\n",
      "     joy  trust  fear  surprise  sadness  disgust  anger  anticipation  \\\n",
      "364    0      0     0         0        0        0      0             1   \n",
      "648    1      0     0         0        0        0      0             0   \n",
      "882    0      0     0         0        0        0      0             0   \n",
      "95     0      0     1         0        0        0      0             0   \n",
      "240    1      0     0         0        0        0      0             0   \n",
      "\n",
      "     neutral  \n",
      "364        0  \n",
      "648        0  \n",
      "882        1  \n",
      "95         0  \n",
      "240        0  \n",
      "['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral']\n",
      "     joy  trust  fear  surprise  sadness  disgust  anger  anticipation  \\\n",
      "873    0      1     0         0        0        0      0             0   \n",
      "686    0      1     0         0        0        0      0             0   \n",
      "274    0      0     0         0        0        0      1             0   \n",
      "136    0      0     1         0        0        0      0             0   \n",
      "264    0      0     1         0        0        0      0             0   \n",
      "\n",
      "     neutral  \n",
      "873        0  \n",
      "686        0  \n",
      "274        0  \n",
      "136        0  \n",
      "264        0  \n",
      "language: en\n",
      "Word Counts: 2530\n",
      "Nrows: 720\n",
      "720 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 16\n",
      "\t95percentile : 29\n",
      "\t99percentile : 40\n",
      "x_train shape: (720,75)\n",
      "y_train shape: (720, 9)\n",
      "Is Multi-Label? False\n",
      "180 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 16\n",
      "\t95percentile : 29\n",
      "\t99percentile : 34\n",
      "x_test shape: (180,75)\n",
      "y_test shape: (180, 9)\n",
      "(720, 75)\n",
      "(720, 9)\n",
      "[[  0   0   0 ...  67 197  67]\n",
      " [  0   0   0 ...   8 290 509]\n",
      " [  0   0   0 ... 673 107  61]\n",
      " ...\n",
      " [  0   0   0 ...   5 178 916]\n",
      " [  0   0   0 ...   1 631  92]\n",
      " [  0   0   0 ...   1 151 910]]\n",
      "{0: 0.7407407407407407, 1: 0.851063829787234, 2: 1.4285714285714286, 3: 2.1621621621621623, 4: 0.8421052631578947, 5: 2.0, 6: 1.951219512195122, 7: 1.0526315789473684, 8: 0.4624277456647399}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 75)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 75, 200)           200000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 75, 64)            59648     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 75, 64)            24832     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32)                10368     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 295,145\n",
      "Trainable params: 295,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ktrain import text\n",
    "\n",
    "input = pd.read_csv('../data/complete_ktrain.csv')\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), preprocessing = text.texts_from_df(train_df=input, text_column='sentence',\n",
    "    label_columns=['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral'],\n",
    "                        val_pct=0.2, max_features=1000, maxlen=75)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train)\n",
    "\n",
    "# generate balanced weights for training\n",
    "from sklearn.utils import class_weight\n",
    "def generate_balanced_weights(y_train):\n",
    "    y_labels = [y.argmax() for y in y_train]\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_labels), y_labels)\n",
    "    weight_dict = {}\n",
    "    for key in range(len(class_weights)):\n",
    "        weight_dict[key] = class_weights[key]\n",
    "    return weight_dict\n",
    "\n",
    "class_weight_dict = generate_balanced_weights(y_train)\n",
    "print(class_weight_dict)\n",
    "\n",
    "# model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "max_length = 75\n",
    "max_words = 1000\n",
    "features = 200\n",
    "classes = 9\n",
    "input_1 = Input(shape=(max_length,))\n",
    "embed_1 = Embedding(input_dim=(max_words), output_dim=features, input_length=max_length)(input_1)\n",
    "bi_lstm_1 = Bidirectional(LSTM(units=32, activation='tanh', dropout=0.2, return_sequences=True))(embed_1)\n",
    "bi_lstm_2 = Bidirectional(LSTM(units=32, activation='tanh', dropout=0.2, return_sequences=True))(bi_lstm_1)\n",
    "bi_lstm_3 = Bidirectional(LSTM(units=16, activation='tanh', dropout=0.2, return_sequences=False))(bi_lstm_2)\n",
    "softmax_1 = Dense(units=classes, activation='softmax')(bi_lstm_3)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=softmax_1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 6s 50ms/step - loss: 2.2037 - accuracy: 0.0944 - val_loss: 2.1935 - val_accuracy: 0.1611\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 2.1725 - accuracy: 0.1653 - val_loss: 2.1754 - val_accuracy: 0.1278\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 2.0263 - accuracy: 0.2222 - val_loss: 2.2012 - val_accuracy: 0.0944\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.8405 - accuracy: 0.2903 - val_loss: 2.2481 - val_accuracy: 0.1167\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.6636 - accuracy: 0.3333 - val_loss: 2.3277 - val_accuracy: 0.1167\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.4848 - accuracy: 0.4139 - val_loss: 2.4721 - val_accuracy: 0.1278\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.2888 - accuracy: 0.5042 - val_loss: 2.6039 - val_accuracy: 0.1889\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.1580 - accuracy: 0.5361 - val_loss: 2.7730 - val_accuracy: 0.1278\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.0193 - accuracy: 0.6000 - val_loss: 2.9529 - val_accuracy: 0.1389\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.8759 - accuracy: 0.6764 - val_loss: 2.9163 - val_accuracy: 0.1833\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.7717 - accuracy: 0.7000 - val_loss: 2.9371 - val_accuracy: 0.1556\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.6646 - accuracy: 0.7583 - val_loss: 3.0969 - val_accuracy: 0.1889\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5728 - accuracy: 0.7931 - val_loss: 3.3224 - val_accuracy: 0.1722\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5303 - accuracy: 0.8222 - val_loss: 3.4458 - val_accuracy: 0.1500\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.4398 - accuracy: 0.8694 - val_loss: 3.5375 - val_accuracy: 0.1722\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.3787 - accuracy: 0.8764 - val_loss: 3.6747 - val_accuracy: 0.1722\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.3321 - accuracy: 0.8972 - val_loss: 3.8405 - val_accuracy: 0.1556\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.3247 - accuracy: 0.9014 - val_loss: 3.7706 - val_accuracy: 0.1833\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.2249 - accuracy: 0.9347 - val_loss: 4.2379 - val_accuracy: 0.1722\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.2152 - accuracy: 0.9292 - val_loss: 4.2019 - val_accuracy: 0.1611\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=16, epochs=20, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_val, y_val))\n",
    "predictor = ktrain.get_predictor(learner.model, preproc=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test_set.csv')\n",
    "sentences = test['sentence'].values\n",
    "labels = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:    My husband has end stage liver disease  and I am so glad I found this place - the info seems really helpful  and the folks seen so understanding\n",
      "Label:  joy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=anticipation\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.363</b>, score <b>-0.508</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.70%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.641\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.149\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 92.77%); opacity: 0.82\" title=\"0.165\">my</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.077\">husband</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.05%); opacity: 0.90\" title=\"0.916\">has</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.73%); opacity: 0.83\" title=\"0.273\">end</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.67%); opacity: 0.86\" title=\"0.530\">stage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.31%); opacity: 0.89\" title=\"0.902\">liver</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.40%); opacity: 0.84\" title=\"-0.408\">disease</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.46%); opacity: 0.85\" title=\"0.494\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.79%); opacity: 0.81\" title=\"0.104\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 74.02%); opacity: 0.91\" title=\"1.029\">am</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.80%); opacity: 0.81\" title=\"-0.133\">so</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.57%); opacity: 0.82\" title=\"-0.172\">glad</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.24%); opacity: 0.81\" title=\"-0.091\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.14%); opacity: 0.82\" title=\"-0.187\">found</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.77%); opacity: 0.83\" title=\"0.310\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.31%); opacity: 0.96\" title=\"1.555\">place</span><span style=\"opacity: 0.80\"> - </span><span style=\"background-color: hsl(120, 100.00%, 86.78%); opacity: 0.84\" title=\"0.392\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.94%); opacity: 0.81\" title=\"0.072\">info</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.02%); opacity: 0.81\" title=\"0.071\">seems</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.95%); opacity: 0.87\" title=\"0.661\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.00%); opacity: 0.80\" title=\"-0.026\">helpful</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.34%); opacity: 0.87\" title=\"0.641\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.68%); opacity: 0.82\" title=\"-0.169\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-1.906\">folks</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.68%); opacity: 0.92\" title=\"1.164\">seen</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.99%); opacity: 0.84\" title=\"-0.383\">so</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.76%); opacity: 0.95\" title=\"1.463\">understanding</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral']\n",
    "index = 1\n",
    "print('Sentence: ', sentences[index])\n",
    "print('Label: ', emotions[labels[index]])\n",
    "predictor.explain(sentences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
