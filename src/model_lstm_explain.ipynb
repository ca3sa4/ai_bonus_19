{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral']\n",
      "     joy  trust  fear  surprise  sadness  disgust  anger  anticipation  \\\n",
      "797    0      0     0         0        0        0      0             1   \n",
      "603    0      1     0         0        0        0      0             0   \n",
      "848    0      0     0         0        1        0      0             0   \n",
      "264    0      0     1         0        0        0      0             0   \n",
      "554    0      0     0         0        0        0      0             0   \n",
      "\n",
      "     neutral  \n",
      "797        0  \n",
      "603        0  \n",
      "848        0  \n",
      "264        0  \n",
      "554        1  \n",
      "['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral']\n",
      "     joy  trust  fear  surprise  sadness  disgust  anger  anticipation  \\\n",
      "568    0      0     0         0        0        0      0             1   \n",
      "615    0      0     1         0        0        0      0             0   \n",
      "745    0      0     0         0        0        0      0             1   \n",
      "706    0      0     0         0        0        0      0             1   \n",
      "66     0      1     0         0        0        0      0             0   \n",
      "\n",
      "     neutral  \n",
      "568        0  \n",
      "615        0  \n",
      "745        0  \n",
      "706        0  \n",
      "66         0  \n",
      "language: en\n",
      "Word Counts: 2510\n",
      "Nrows: 720\n",
      "720 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 16\n",
      "\t95percentile : 30\n",
      "\t99percentile : 39\n",
      "x_train shape: (720,75)\n",
      "y_train shape: (720, 9)\n",
      "Is Multi-Label? False\n",
      "180 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 15\n",
      "\t95percentile : 28\n",
      "\t99percentile : 36\n",
      "x_test shape: (180,75)\n",
      "y_test shape: (180, 9)\n",
      "(720, 75)\n",
      "(720, 9)\n",
      "[[  0   0   0 ...  31 666 148]\n",
      " [  0   0   0 ...   1   2 111]\n",
      " [  0   0   0 ...   5 232  24]\n",
      " ...\n",
      " [  0   0   0 ...   1 134  13]\n",
      " [  0   0   0 ... 252   7 263]\n",
      " [  0   0   0 ... 188  13 608]]\n",
      "{0: 0.7339449541284404, 1: 0.8421052631578947, 2: 1.3333333333333333, 3: 2.0, 4: 0.8791208791208791, 5: 2.0, 6: 1.8604651162790697, 7: 1.0810810810810811, 8: 0.47619047619047616}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 75)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 75, 200)           200000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 75, 64)            59648     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 75, 64)            24832     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32)                10368     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 295,145\n",
      "Trainable params: 295,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ktrain import text\n",
    "\n",
    "input = pd.read_csv('../data/complete_ktrain.csv')\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), preprocessing = text.texts_from_df(train_df=input, text_column='sentence',\n",
    "    label_columns=['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral'],\n",
    "                        val_pct=0.2, max_features=1000, maxlen=75)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train)\n",
    "\n",
    "# generate balanced weights for training\n",
    "from sklearn.utils import class_weight\n",
    "def generate_balanced_weights(y_train):\n",
    "    y_labels = [y.argmax() for y in y_train]\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_labels), y_labels)\n",
    "    weight_dict = {}\n",
    "    for key in range(len(class_weights)):\n",
    "        weight_dict[key] = class_weights[key]\n",
    "    return weight_dict\n",
    "\n",
    "class_weight_dict = generate_balanced_weights(y_train)\n",
    "print(class_weight_dict)\n",
    "\n",
    "# model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "max_length = 75\n",
    "max_words = 1000\n",
    "features = 200\n",
    "classes = 9\n",
    "input_1 = Input(shape=(max_length,))\n",
    "embed_1 = Embedding(input_dim=(max_words), output_dim=features, input_length=max_length)(input_1)\n",
    "bi_lstm_1 = Bidirectional(LSTM(units=32, activation='tanh', dropout=0.2, return_sequences=True))(embed_1)\n",
    "bi_lstm_2 = Bidirectional(LSTM(units=32, activation='tanh', dropout=0.2, return_sequences=True))(bi_lstm_1)\n",
    "bi_lstm_3 = Bidirectional(LSTM(units=16, activation='tanh', dropout=0.2, return_sequences=False))(bi_lstm_2)\n",
    "softmax_1 = Dense(units=classes, activation='softmax')(bi_lstm_3)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=softmax_1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 6s 50ms/step - loss: 2.2044 - accuracy: 0.0819 - val_loss: 2.1973 - val_accuracy: 0.0778\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 2.1803 - accuracy: 0.1597 - val_loss: 2.1946 - val_accuracy: 0.1056\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 2.0374 - accuracy: 0.1944 - val_loss: 2.2226 - val_accuracy: 0.0833\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.8396 - accuracy: 0.2542 - val_loss: 2.3148 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 1.6534 - accuracy: 0.3486 - val_loss: 2.4426 - val_accuracy: 0.1111\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 1.5283 - accuracy: 0.3750 - val_loss: 2.5641 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.3815 - accuracy: 0.4194 - val_loss: 2.7404 - val_accuracy: 0.1722\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 1.3010 - accuracy: 0.4597 - val_loss: 2.7473 - val_accuracy: 0.1722\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.1930 - accuracy: 0.5028 - val_loss: 2.9693 - val_accuracy: 0.1222\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 1.1086 - accuracy: 0.5125 - val_loss: 3.1025 - val_accuracy: 0.1444\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.0270 - accuracy: 0.5722 - val_loss: 3.0311 - val_accuracy: 0.1778\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.9586 - accuracy: 0.6236 - val_loss: 3.2971 - val_accuracy: 0.1500\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.8492 - accuracy: 0.6667 - val_loss: 3.3448 - val_accuracy: 0.1333\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.7964 - accuracy: 0.6792 - val_loss: 3.5357 - val_accuracy: 0.1444\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.6633 - accuracy: 0.7819 - val_loss: 3.6452 - val_accuracy: 0.1389\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.6341 - accuracy: 0.7889 - val_loss: 3.6131 - val_accuracy: 0.1278\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5623 - accuracy: 0.8125 - val_loss: 3.8612 - val_accuracy: 0.1222\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.5059 - accuracy: 0.8236 - val_loss: 3.7871 - val_accuracy: 0.1333\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.4782 - accuracy: 0.8486 - val_loss: 3.8462 - val_accuracy: 0.1556\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 0.4097 - accuracy: 0.8722 - val_loss: 3.9941 - val_accuracy: 0.1389\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=16, epochs=20, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_val, y_val))\n",
    "predictor = ktrain.get_predictor(learner.model, preproc=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test_set.csv')\n",
    "sentences = test['sentence'].values\n",
    "labels = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:   We love the heat  and both themister and I were one's who lived for the sun  pool parties  laying out with baby oil\n",
      "Label:  joy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=joy\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.952</b>, score <b>3.637</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.621\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.22%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.984\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 97.50%); opacity: 0.80\" title=\"0.019\">we</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.994\">love</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.01%); opacity: 0.81\" title=\"0.037\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.16%); opacity: 0.83\" title=\"0.134\">heat</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.42%); opacity: 0.85\" title=\"0.235\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.32%); opacity: 0.92\" title=\"0.587\">both</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.32%); opacity: 0.96\" title=\"0.777\">themister</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.61%); opacity: 0.91\" title=\"0.549\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.67%); opacity: 0.87\" title=\"0.326\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.45%); opacity: 0.85\" title=\"0.234\">were</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 61.76%); opacity: 0.99\" title=\"0.932\">one</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(120, 100.00%, 87.53%); opacity: 0.84\" title=\"0.188\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.42%); opacity: 0.92\" title=\"0.584\">who</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.77%); opacity: 0.89\" title=\"0.457\">lived</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.92%); opacity: 0.90\" title=\"0.481\">for</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.11%); opacity: 0.84\" title=\"0.197\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.86%); opacity: 0.81\" title=\"0.053\">sun</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.41%); opacity: 0.84\" title=\"0.213\">pool</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.82%); opacity: 0.84\" title=\"0.203\">parties</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.65%); opacity: 0.84\" title=\"0.185\">laying</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.59%); opacity: 0.94\" title=\"0.703\">out</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.01%); opacity: 0.90\" title=\"0.507\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.97%); opacity: 0.86\" title=\"0.293\">baby</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.62%); opacity: 0.82\" title=\"0.106\">oil</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation', 'neutral']\n",
    "index = 28\n",
    "print('Sentence: ', sentences[index])\n",
    "print('Label: ', emotions[labels[index]])\n",
    "predictor.explain(sentences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
